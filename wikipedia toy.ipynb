{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Toy')  \n",
    "article = scraped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:  \n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Square Brackets and Extra Spaces\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(article_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in nltk.word_tokenize(formatted_article_text):  \n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():  \n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The materials that toys are made from have changed, what toys can do has changed, but the fact that children play with toys has not. Dinky Toys pioneered the manufacture of die-cast toys with the production of toy cars, trains and ships and model train sets became popular in the 1920s. Sometimes intended as decorations, keepsakes, or collectibles for older children and adults, most dolls are intended as toys for children, usually girls, to play with. Toy companies have often promoted the segregation by gender in toys because it enables them to customize the same toy for each gender, which ultimately doubles their revenue. One example of the dramatic ways that toys can influence child development involves clay sculpting toys such as Play-Doh and Silly Putty and their home-made counterparts. Children, especially very small ones, often put toys into their mouths, so the materials used to make a toy are regulated to prevent poisoning. In 2007, massive recalls of toys produced in China led many U.S.-based charities to cut back on, or even discontinue, their acceptance of used toys.\n"
     ]
    }
   ],
   "source": [
    "import heapq  \n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)  \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 94)\n",
      "\n",
      "\n",
      "                                                Idea  Cluster\n",
      "0            She went to the airport to see him off.        0\n",
      "0                       I prefer reading to writing.        0\n",
      "4  Los Angeles is in California. Its southeast of...        4\n",
      "0                   I ate a burger then went to bed.        0\n",
      "4                     Compare your answer with Toms.        4\n",
      "1  I had hardly left home when it began to rain h...        1\n",
      "1  If he had asked me, I would have given it to h...        1\n",
      "1  I could have come by auto, but who would pay t...        1\n",
      "3  Whatever it may be, you should not have beaten...        3\n",
      "3                  You should have told me yesterday        3\n",
      "3        I should have joined this course last year.        3\n",
      "2                               Where are you going?        2\n",
      "2                    There are too many people here.        2\n",
      "1                      Everyone always asks me that.        1\n",
      "2           I didnt think you were going to make it.        2\n",
      "3                      Be quiet while I am speaking.        3\n",
      "1                  I cant figure out why he said so.        1\n",
      "\n",
      "\n",
      "1    5\n",
      "3    4\n",
      "2    3\n",
      "0    3\n",
      "4    2\n",
      "Name: Cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''data=pd.read_excel('data.xlsx') #Include your data file instead of data.xlsx\n",
    "idea=data.iloc[:,0:1] #Selecting the first column that has text.\n",
    "\n",
    "#Converting the column of data from excel sheet into a list of documents, where each document corresponds to a group of sentences.\n",
    "corpus=[]\n",
    "for index,row in idea.iterrows():\n",
    "    corpus.append(row['Idea'])\n",
    "Or you could just comment out the above code and use this dummy corpus list instead if you don't have the data.'''\n",
    "corpus=['She went to the airport to see him off.','I prefer reading to writing.','Los Angeles is in California. Its southeast of San Francisco.','I ate a burger then went to bed.','Compare your answer with Toms.','I had hardly left home when it began to rain heavily.','If he had asked me, I would have given it to him. ','I could have come by auto, but who would pay the fare? ','Whatever it may be, you should not have beaten him.','You should have told me yesterday','I should have joined this course last year.','Where are you going?','There are too many people here.','Everyone always asks me that.','I didnt think you were going to make it.','Be quiet while I am speaking.','I cant figure out why he said so.']\n",
    "    \n",
    "    \n",
    "#Count Vectoriser then tidf transformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#vectorizer.get_feature_names()\n",
    "\n",
    "#print(X.toarray())     \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print(tfidf.shape )                        \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5 #Change it according to your data.\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "idea={'Idea':corpus, 'Cluster':clusters} #Creating dict having doc with the corresponding cluster number.\n",
    "frame=pd.DataFrame(idea,index=[clusters], columns=['Idea','Cluster']) # Converting it into a dataframe.\n",
    "\n",
    "print(\"\\n\")\n",
    "print(frame) #Print the doc with the labeled cluster number.\n",
    "print(\"\\n\")\n",
    "print(frame['Cluster'].value_counts()) #Print the counts of doc belonging to each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
